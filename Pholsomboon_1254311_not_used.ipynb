{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee10a168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/wiwatp/.cache/kagglehub/datasets/miguelcorraljr/ted-ultimate-dataset/versions/2\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "RANDOM_STATE = 555\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"miguelcorraljr/ted-ultimate-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "en_datapath = os.path.join(path ,\"2020-05-01\", \"ted_talks_en.csv\")\n",
    "df = pd.read_csv(en_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86aacb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['transcript_length'] = df['transcript'].apply(len)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df['transcript_length'], bins=50)\n",
    "plt.title(\"Length of Transcripts (English)\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c016b5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>talk_id</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker_1</th>\n",
       "      <th>all_speakers</th>\n",
       "      <th>occupations</th>\n",
       "      <th>about_speakers</th>\n",
       "      <th>views</th>\n",
       "      <th>recorded_date</th>\n",
       "      <th>published_date</th>\n",
       "      <th>event</th>\n",
       "      <th>native_lang</th>\n",
       "      <th>available_lang</th>\n",
       "      <th>comments</th>\n",
       "      <th>duration</th>\n",
       "      <th>topics</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>transcript</th>\n",
       "      <th>transcript_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>{0: 'Al Gore'}</td>\n",
       "      <td>{0: ['climate advocate']}</td>\n",
       "      <td>{0: 'Nobel Laureate Al Gore focused the world’...</td>\n",
       "      <td>3523392</td>\n",
       "      <td>2006-02-25</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'cs', 'de', 'el', 'en', 'es', 'fa...</td>\n",
       "      <td>272.0</td>\n",
       "      <td>977</td>\n",
       "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
       "      <td>{243: 'New thinking on the climate crisis', 54...</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_averting_the...</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "      <td>11878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>Hans Rosling</td>\n",
       "      <td>{0: 'Hans Rosling'}</td>\n",
       "      <td>{0: ['global health expert; data visionary']}</td>\n",
       "      <td>{0: 'In Hans Rosling’s hands, data sings. Glob...</td>\n",
       "      <td>14501685</td>\n",
       "      <td>2006-02-22</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'az', 'bg', 'bn', 'bs', 'cs', 'da', 'de...</td>\n",
       "      <td>628.0</td>\n",
       "      <td>1190</td>\n",
       "      <td>['Africa', 'Asia', 'Google', 'demo', 'economic...</td>\n",
       "      <td>{2056: \"Own your body's data\", 2296: 'A visual...</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_the_bes...</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "      <td>17453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>{0: 'David Pogue'}</td>\n",
       "      <td>{0: ['technology columnist']}</td>\n",
       "      <td>{0: 'David Pogue is the personal technology co...</td>\n",
       "      <td>1920832</td>\n",
       "      <td>2006-02-24</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'de', 'el', 'en', 'es', 'fa', 'fr...</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1286</td>\n",
       "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
       "      <td>{1725: '10 top time-saving tech tips', 2274: '...</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_simplici...</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "      <td>18427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>Majora Carter</td>\n",
       "      <td>{0: 'Majora Carter'}</td>\n",
       "      <td>{0: ['activist for environmental justice']}</td>\n",
       "      <td>{0: 'Majora Carter redefined the field of envi...</td>\n",
       "      <td>2664069</td>\n",
       "      <td>2006-02-26</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['ar', 'bg', 'bn', 'ca', 'cs', 'de', 'en', 'es...</td>\n",
       "      <td>219.0</td>\n",
       "      <td>1116</td>\n",
       "      <td>['MacArthur grant', 'activism', 'business', 'c...</td>\n",
       "      <td>{1041: '3 stories of local eco-entrepreneurshi...</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_greeni...</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>If you're here today — and I'm very happy that...</td>\n",
       "      <td>18305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>Sir Ken Robinson</td>\n",
       "      <td>{0: 'Sir Ken Robinson'}</td>\n",
       "      <td>{0: ['author', 'educator']}</td>\n",
       "      <td>{0: \"Creativity expert Sir Ken Robinson challe...</td>\n",
       "      <td>65051954</td>\n",
       "      <td>2006-02-25</td>\n",
       "      <td>2006-06-27</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>en</td>\n",
       "      <td>['af', 'ar', 'az', 'be', 'bg', 'bn', 'ca', 'cs...</td>\n",
       "      <td>4931.0</td>\n",
       "      <td>1164</td>\n",
       "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
       "      <td>{865: 'Bring on the learning revolution!', 173...</td>\n",
       "      <td>https://www.ted.com/talks/sir_ken_robinson_do_...</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>Good morning. How are you? (Audience) Good. It...</td>\n",
       "      <td>17558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   talk_id                            title         speaker_1  \\\n",
       "0        1      Averting the climate crisis           Al Gore   \n",
       "1       92  The best stats you've ever seen      Hans Rosling   \n",
       "2        7                 Simplicity sells       David Pogue   \n",
       "3       53              Greening the ghetto     Majora Carter   \n",
       "4       66      Do schools kill creativity?  Sir Ken Robinson   \n",
       "\n",
       "              all_speakers                                    occupations  \\\n",
       "0           {0: 'Al Gore'}                      {0: ['climate advocate']}   \n",
       "1      {0: 'Hans Rosling'}  {0: ['global health expert; data visionary']}   \n",
       "2       {0: 'David Pogue'}                  {0: ['technology columnist']}   \n",
       "3     {0: 'Majora Carter'}    {0: ['activist for environmental justice']}   \n",
       "4  {0: 'Sir Ken Robinson'}                    {0: ['author', 'educator']}   \n",
       "\n",
       "                                      about_speakers     views recorded_date  \\\n",
       "0  {0: 'Nobel Laureate Al Gore focused the world’...   3523392    2006-02-25   \n",
       "1  {0: 'In Hans Rosling’s hands, data sings. Glob...  14501685    2006-02-22   \n",
       "2  {0: 'David Pogue is the personal technology co...   1920832    2006-02-24   \n",
       "3  {0: 'Majora Carter redefined the field of envi...   2664069    2006-02-26   \n",
       "4  {0: \"Creativity expert Sir Ken Robinson challe...  65051954    2006-02-25   \n",
       "\n",
       "  published_date    event native_lang  \\\n",
       "0     2006-06-27  TED2006          en   \n",
       "1     2006-06-27  TED2006          en   \n",
       "2     2006-06-27  TED2006          en   \n",
       "3     2006-06-27  TED2006          en   \n",
       "4     2006-06-27  TED2006          en   \n",
       "\n",
       "                                      available_lang  comments  duration  \\\n",
       "0  ['ar', 'bg', 'cs', 'de', 'el', 'en', 'es', 'fa...     272.0       977   \n",
       "1  ['ar', 'az', 'bg', 'bn', 'bs', 'cs', 'da', 'de...     628.0      1190   \n",
       "2  ['ar', 'bg', 'de', 'el', 'en', 'es', 'fa', 'fr...     124.0      1286   \n",
       "3  ['ar', 'bg', 'bn', 'ca', 'cs', 'de', 'en', 'es...     219.0      1116   \n",
       "4  ['af', 'ar', 'az', 'be', 'bg', 'bn', 'ca', 'cs...    4931.0      1164   \n",
       "\n",
       "                                              topics  \\\n",
       "0  ['alternative energy', 'cars', 'climate change...   \n",
       "1  ['Africa', 'Asia', 'Google', 'demo', 'economic...   \n",
       "2  ['computers', 'entertainment', 'interface desi...   \n",
       "3  ['MacArthur grant', 'activism', 'business', 'c...   \n",
       "4  ['children', 'creativity', 'culture', 'dance',...   \n",
       "\n",
       "                                       related_talks  \\\n",
       "0  {243: 'New thinking on the climate crisis', 54...   \n",
       "1  {2056: \"Own your body's data\", 2296: 'A visual...   \n",
       "2  {1725: '10 top time-saving tech tips', 2274: '...   \n",
       "3  {1041: '3 stories of local eco-entrepreneurshi...   \n",
       "4  {865: 'Bring on the learning revolution!', 173...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.ted.com/talks/al_gore_averting_the...   \n",
       "1  https://www.ted.com/talks/hans_rosling_the_bes...   \n",
       "2  https://www.ted.com/talks/david_pogue_simplici...   \n",
       "3  https://www.ted.com/talks/majora_carter_greeni...   \n",
       "4  https://www.ted.com/talks/sir_ken_robinson_do_...   \n",
       "\n",
       "                                         description  \\\n",
       "0  With the same humor and humanity he exuded in ...   \n",
       "1  You've never seen data presented like this. Wi...   \n",
       "2  New York Times columnist David Pogue takes aim...   \n",
       "3  In an emotionally charged talk, MacArthur-winn...   \n",
       "4  Sir Ken Robinson makes an entertaining and pro...   \n",
       "\n",
       "                                          transcript  transcript_length  \n",
       "0  Thank you so much, Chris. And it's truly a gre...              11878  \n",
       "1  About 10 years ago, I took on the task to teac...              17453  \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...              18427  \n",
       "3  If you're here today — and I'm very happy that...              18305  \n",
       "4  Good morning. How are you? (Audience) Good. It...              17558  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40bc8ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4005 entries, 0 to 4004\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   talk_id            4005 non-null   int64  \n",
      " 1   title              4005 non-null   object \n",
      " 2   speaker_1          4005 non-null   object \n",
      " 3   all_speakers       4001 non-null   object \n",
      " 4   occupations        3483 non-null   object \n",
      " 5   about_speakers     3502 non-null   object \n",
      " 6   views              4005 non-null   int64  \n",
      " 7   recorded_date      4004 non-null   object \n",
      " 8   published_date     4005 non-null   object \n",
      " 9   event              4005 non-null   object \n",
      " 10  native_lang        4005 non-null   object \n",
      " 11  available_lang     4005 non-null   object \n",
      " 12  comments           3350 non-null   float64\n",
      " 13  duration           4005 non-null   int64  \n",
      " 14  topics             4005 non-null   object \n",
      " 15  related_talks      4005 non-null   object \n",
      " 16  url                4005 non-null   object \n",
      " 17  description        4005 non-null   object \n",
      " 18  transcript         4005 non-null   object \n",
      " 19  transcript_length  4005 non-null   int64  \n",
      "dtypes: float64(1), int64(4), object(15)\n",
      "memory usage: 625.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6c2c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_columns = ['talk_id',\n",
    "                  'title',\n",
    "                  'speaker_1',\n",
    "                  'all_speakers',\n",
    "                  'occupations',\n",
    "                  'about_speakers',\n",
    "                  'views',\n",
    "                  'recorded_date',\n",
    "                  'published_date',\n",
    "                  'event',\n",
    "                  'duration',\n",
    "                  'topics',\n",
    "                  'related_talks',\n",
    "                  'url']\n",
    "\n",
    "df.drop(columns=unused_columns, inplace=True, errors='ignore'  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee95f699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def process_text(text):\n",
    "    doc = nlp(text[:1000])\n",
    "    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "df['processed'] = df['transcript'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b8ba487",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_join'] = df['processed'].apply(lambda x:' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95c9411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('dataset'):   \n",
    "    os.makedirs('dataset')\n",
    "    \n",
    "# create corpus.tsv\n",
    "df[['processed_join']].to_csv(os.path.join('dataset', 'corpus.tsv'), sep='\\t', index=False, header=False)\n",
    "# create vocabulary.txt\n",
    "# with open(os.path.join('dataset', 'vocabulary.txt'), 'w') as f:\n",
    "#     for token in df['processed'].explode().unique():\n",
    "#         f.write(f\"{token}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5711833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octis.dataset.dataset import Dataset\n",
    "\n",
    "dataset = Dataset() \n",
    "dataset.load_custom_dataset_from_folder(path='dataset') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "488672bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octis.models.LDA import LDA\n",
    "lda = LDA(random_state=RANDOM_STATE)\n",
    "lda_model = lda.train_model(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f9c4816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['brave',\n",
       "  'Grand',\n",
       "  'act',\n",
       "  'state',\n",
       "  'late',\n",
       "  'service',\n",
       "  'Alabama',\n",
       "  'century',\n",
       "  'spring',\n",
       "  'play'],\n",
       " ['brain',\n",
       "  'know',\n",
       "  'like',\n",
       "  'object',\n",
       "  'go',\n",
       "  'time',\n",
       "  'right',\n",
       "  'let',\n",
       "  'year',\n",
       "  'human'],\n",
       " ['test',\n",
       "  'population',\n",
       "  'multiply',\n",
       "  'attention',\n",
       "  'ability',\n",
       "  'assess',\n",
       "  'design',\n",
       "  'result',\n",
       "  'basis',\n",
       "  'divide'],\n",
       " ['blood',\n",
       "  'plant',\n",
       "  'creature',\n",
       "  'process',\n",
       "  'chamber',\n",
       "  'tall',\n",
       "  'mistake',\n",
       "  'live',\n",
       "  'come',\n",
       "  'like'],\n",
       " ['like',\n",
       "  'people',\n",
       "  'go',\n",
       "  'world',\n",
       "  'family',\n",
       "  'time',\n",
       "  'know',\n",
       "  'pirate',\n",
       "  'thing',\n",
       "  'Yale'],\n",
       " ['car',\n",
       "  'district',\n",
       "  'attorney',\n",
       "  'permanently',\n",
       "  'charge',\n",
       "  'order',\n",
       "  'file',\n",
       "  'general',\n",
       "  'find',\n",
       "  'answer'],\n",
       " ['gradual',\n",
       "  'Alabama',\n",
       "  'fight',\n",
       "  'scorn',\n",
       "  'bear',\n",
       "  'sell',\n",
       "  'spring',\n",
       "  '♫',\n",
       "  'power',\n",
       "  'York'],\n",
       " ['atom',\n",
       "  'story',\n",
       "  'know',\n",
       "  'time',\n",
       "  'tell',\n",
       "  '♫',\n",
       "  'year',\n",
       "  'small',\n",
       "  'comic',\n",
       "  'butterfly'],\n",
       " ['physics',\n",
       "  'satellite',\n",
       "  'Newton',\n",
       "  'Earth',\n",
       "  'intersection',\n",
       "  'time',\n",
       "  'like',\n",
       "  'gay',\n",
       "  'equal',\n",
       "  'leader'],\n",
       " ['careful',\n",
       "  'like',\n",
       "  'police',\n",
       "  'news',\n",
       "  'tell',\n",
       "  'help',\n",
       "  'share',\n",
       "  'sex',\n",
       "  'think',\n",
       "  'hear'],\n",
       " ['adhesive',\n",
       "  'daily',\n",
       "  'cylindrical',\n",
       "  'entire',\n",
       "  'flow',\n",
       "  'sea',\n",
       "  'fact',\n",
       "  'simply',\n",
       "  'like',\n",
       "  'body'],\n",
       " ['planet',\n",
       "  'star',\n",
       "  'life',\n",
       "  'know',\n",
       "  'earth',\n",
       "  'rope',\n",
       "  'like',\n",
       "  'want',\n",
       "  'go',\n",
       "  'water'],\n",
       " ['protein',\n",
       "  'crop',\n",
       "  'thing',\n",
       "  'depression',\n",
       "  'life',\n",
       "  'carbohydrate',\n",
       "  'flower',\n",
       "  'like',\n",
       "  'chinese',\n",
       "  'fruit'],\n",
       " ['mid-20',\n",
       "  'York',\n",
       "  'New',\n",
       "  'hold',\n",
       "  'money',\n",
       "  'month',\n",
       "  'system',\n",
       "  'phone',\n",
       "  'year',\n",
       "  'life'],\n",
       " ['energy',\n",
       "  'know',\n",
       "  'story',\n",
       "  'attention',\n",
       "  'travel',\n",
       "  'today',\n",
       "  'laughter',\n",
       "  'movie',\n",
       "  'year',\n",
       "  'daughter'],\n",
       " ['place',\n",
       "  'year',\n",
       "  'time',\n",
       "  'like',\n",
       "  'know',\n",
       "  'world',\n",
       "  'look',\n",
       "  'live',\n",
       "  'life',\n",
       "  'value'],\n",
       " ['man',\n",
       "  'like',\n",
       "  'help',\n",
       "  'police',\n",
       "  'tell',\n",
       "  'girl',\n",
       "  'danger',\n",
       "  'spiritual',\n",
       "  'quotient',\n",
       "  'cognitive'],\n",
       " ['faith',\n",
       "  'dedicate',\n",
       "  'understanding',\n",
       "  'approach',\n",
       "  'sell',\n",
       "  'currently',\n",
       "  'eye',\n",
       "  'far',\n",
       "  'money',\n",
       "  'play'],\n",
       " ['slavery',\n",
       "  'period',\n",
       "  'finally',\n",
       "  'court',\n",
       "  'York',\n",
       "  'New',\n",
       "  'eye',\n",
       "  'return',\n",
       "  'hold',\n",
       "  'play'],\n",
       " ['crime',\n",
       "  'blink',\n",
       "  'road',\n",
       "  'car',\n",
       "  'drive',\n",
       "  'officer',\n",
       "  'file',\n",
       "  'charge',\n",
       "  'happen',\n",
       "  'dollar'],\n",
       " ['rock',\n",
       "  'like',\n",
       "  'radiation',\n",
       "  'thing',\n",
       "  'music',\n",
       "  'think',\n",
       "  'time',\n",
       "  'good',\n",
       "  'ocean',\n",
       "  'go'],\n",
       " ['work',\n",
       "  'go',\n",
       "  'year',\n",
       "  'people',\n",
       "  'world',\n",
       "  'laughter',\n",
       "  'day',\n",
       "  'know',\n",
       "  'run',\n",
       "  'like'],\n",
       " ['foot',\n",
       "  'look',\n",
       "  'marine',\n",
       "  'mouth',\n",
       "  'thrive',\n",
       "  'guess',\n",
       "  'sea',\n",
       "  'shape',\n",
       "  'like',\n",
       "  'form'],\n",
       " ['paperwork',\n",
       "  'declare',\n",
       "  'mountain',\n",
       "  'head',\n",
       "  'world',\n",
       "  'find',\n",
       "  'go',\n",
       "  'people',\n",
       "  'year',\n",
       "  'large'],\n",
       " ['particle',\n",
       "  'ice',\n",
       "  'drink',\n",
       "  'physicist',\n",
       "  'contract',\n",
       "  'year',\n",
       "  'alcohol',\n",
       "  'permanent',\n",
       "  'elementary',\n",
       "  'beverage'],\n",
       " ['like',\n",
       "  'laughter',\n",
       "  'time',\n",
       "  'year',\n",
       "  'sleep',\n",
       "  'people',\n",
       "  'world',\n",
       "  'disease',\n",
       "  'think',\n",
       "  'thing'],\n",
       " ['Simon',\n",
       "  'age',\n",
       "  'skill',\n",
       "  '100',\n",
       "  'child',\n",
       "  'indenture',\n",
       "  'Truth',\n",
       "  'g',\n",
       "  'method',\n",
       "  'York'],\n",
       " ['egg',\n",
       "  'come',\n",
       "  'time',\n",
       "  'thing',\n",
       "  'know',\n",
       "  'go',\n",
       "  'laughter',\n",
       "  'like',\n",
       "  'say',\n",
       "  'look'],\n",
       " ['seize',\n",
       "  'spot',\n",
       "  'pull',\n",
       "  'mountain',\n",
       "  'dollar',\n",
       "  'large',\n",
       "  'eye',\n",
       "  'car',\n",
       "  'day',\n",
       "  'find'],\n",
       " ['liquor',\n",
       "  'balloon',\n",
       "  'digestive',\n",
       "  'length',\n",
       "  'Nancy',\n",
       "  'health',\n",
       "  'ecosystem',\n",
       "  'breath',\n",
       "  'like',\n",
       "  'entire'],\n",
       " ['universe',\n",
       "  'know',\n",
       "  'year',\n",
       "  'life',\n",
       "  'time',\n",
       "  'report',\n",
       "  'like',\n",
       "  'water',\n",
       "  'day',\n",
       "  'world'],\n",
       " ['sea',\n",
       "  'generally',\n",
       "  'surround',\n",
       "  'specie',\n",
       "  'look',\n",
       "  'breath',\n",
       "  'essentially',\n",
       "  'flow',\n",
       "  'ecosystem',\n",
       "  'body'],\n",
       " ['people',\n",
       "  'laughter',\n",
       "  'weather',\n",
       "  'like',\n",
       "  'know',\n",
       "  'talk',\n",
       "  'go',\n",
       "  'death',\n",
       "  'think',\n",
       "  'year'],\n",
       " ['intelligence',\n",
       "  'legal',\n",
       "  'France',\n",
       "  '6',\n",
       "  'rescue',\n",
       "  'free',\n",
       "  'rest',\n",
       "  'late',\n",
       "  'century',\n",
       "  'return'],\n",
       " ['AI',\n",
       "  'motion',\n",
       "  'go',\n",
       "  'right',\n",
       "  'fold',\n",
       "  'say',\n",
       "  'paper',\n",
       "  'thing',\n",
       "  'know',\n",
       "  'breath'],\n",
       " ['track',\n",
       "  'water',\n",
       "  'music',\n",
       "  'fool',\n",
       "  'way',\n",
       "  'year',\n",
       "  'think',\n",
       "  'Venus',\n",
       "  'part',\n",
       "  'batter'],\n",
       " ['feeding',\n",
       "  'humble',\n",
       "  'odd',\n",
       "  'like',\n",
       "  'long',\n",
       "  'fact',\n",
       "  'member',\n",
       "  'look',\n",
       "  'way',\n",
       "  'year'],\n",
       " ['brain',\n",
       "  'like',\n",
       "  'year',\n",
       "  'human',\n",
       "  'change',\n",
       "  'theory',\n",
       "  'time',\n",
       "  'woman',\n",
       "  'look',\n",
       "  'people'],\n",
       " ['hurry',\n",
       "  'fuzzy',\n",
       "  'girl',\n",
       "  'sound',\n",
       "  'like',\n",
       "  'San',\n",
       "  'tell',\n",
       "  'safe',\n",
       "  'black',\n",
       "  'flow'],\n",
       " ['24/7',\n",
       "  'Jose',\n",
       "  'pave',\n",
       "  'man',\n",
       "  'thank',\n",
       "  'sound',\n",
       "  'ceo',\n",
       "  'help',\n",
       "  'news',\n",
       "  'mental'],\n",
       " ['tube',\n",
       "  'cofounder',\n",
       "  'rare',\n",
       "  'actual',\n",
       "  'Text',\n",
       "  'simply',\n",
       "  'essentially',\n",
       "  'like',\n",
       "  'look',\n",
       "  'drug'],\n",
       " ['Empire',\n",
       "  'spelling',\n",
       "  'turtle',\n",
       "  'West',\n",
       "  'grief',\n",
       "  'toxic',\n",
       "  'know',\n",
       "  'joint',\n",
       "  'man',\n",
       "  'fierce'],\n",
       " ['cash',\n",
       "  'drug',\n",
       "  'sock',\n",
       "  'electric',\n",
       "  'tomorrow',\n",
       "  'later',\n",
       "  'local',\n",
       "  'money',\n",
       "  'eye',\n",
       "  'stop'],\n",
       " ['justice',\n",
       "  'trial',\n",
       "  'announce',\n",
       "  'son',\n",
       "  'New',\n",
       "  'truth',\n",
       "  'understanding',\n",
       "  'ignore',\n",
       "  'York',\n",
       "  'lawyer'],\n",
       " ['fund',\n",
       "  'serve',\n",
       "  'lawyer',\n",
       "  'York',\n",
       "  'New',\n",
       "  'County',\n",
       "  'month',\n",
       "  'early',\n",
       "  'power',\n",
       "  'act'],\n",
       " ['know',\n",
       "  'woman',\n",
       "  'people',\n",
       "  'come',\n",
       "  'world',\n",
       "  'man',\n",
       "  'movie',\n",
       "  'thing',\n",
       "  'bubble',\n",
       "  'think'],\n",
       " ['spatial',\n",
       "  'ability',\n",
       "  'sample',\n",
       "  'design',\n",
       "  'test',\n",
       "  'basis',\n",
       "  'memory',\n",
       "  'today',\n",
       "  'age',\n",
       "  'like'],\n",
       " ['let',\n",
       "  'go',\n",
       "  'change',\n",
       "  'people',\n",
       "  'year',\n",
       "  'look',\n",
       "  'apple',\n",
       "  'disease',\n",
       "  'power',\n",
       "  'like'],\n",
       " ['♫',\n",
       "  'cat',\n",
       "  'toilet',\n",
       "  'sentence',\n",
       "  'dog',\n",
       "  'day',\n",
       "  'garbage',\n",
       "  'think',\n",
       "  'know',\n",
       "  'go'],\n",
       " ['text',\n",
       "  'verbal',\n",
       "  'banana',\n",
       "  'send',\n",
       "  'arrest',\n",
       "  'LOL',\n",
       "  'handlebar',\n",
       "  'member',\n",
       "  'battery',\n",
       "  'kill'],\n",
       " ['laughter',\n",
       "  'like',\n",
       "  'go',\n",
       "  'car',\n",
       "  'year',\n",
       "  'people',\n",
       "  'think',\n",
       "  'little',\n",
       "  'Henry',\n",
       "  'life'],\n",
       " ['outlet',\n",
       "  'amount',\n",
       "  'suspect',\n",
       "  'convict',\n",
       "  'officer',\n",
       "  'declare',\n",
       "  'fast',\n",
       "  'find',\n",
       "  'report',\n",
       "  'deal'],\n",
       " ['city',\n",
       "  'people',\n",
       "  'war',\n",
       "  'public',\n",
       "  'think',\n",
       "  'space',\n",
       "  'portal',\n",
       "  'building',\n",
       "  'create',\n",
       "  'place'],\n",
       " ['Chicken',\n",
       "  'art',\n",
       "  'look',\n",
       "  'pain',\n",
       "  'actually',\n",
       "  'record',\n",
       "  'artist',\n",
       "  'year',\n",
       "  'punishment',\n",
       "  'book'],\n",
       " ['go',\n",
       "  'like',\n",
       "  'molecule',\n",
       "  'think',\n",
       "  'time',\n",
       "  'actually',\n",
       "  'design',\n",
       "  'world',\n",
       "  'surface',\n",
       "  'matter'],\n",
       " ['Line',\n",
       "  'truthfully',\n",
       "  '14',\n",
       "  'like',\n",
       "  'ok',\n",
       "  'man',\n",
       "  'laughter',\n",
       "  'safe',\n",
       "  'country',\n",
       "  'want'],\n",
       " ['like',\n",
       "  'water',\n",
       "  'people',\n",
       "  'family',\n",
       "  'find',\n",
       "  'time',\n",
       "  'country',\n",
       "  'life',\n",
       "  'year',\n",
       "  'want'],\n",
       " ['come',\n",
       "  'body',\n",
       "  'security',\n",
       "  'day',\n",
       "  'like',\n",
       "  'sugar',\n",
       "  'right',\n",
       "  'force',\n",
       "  'human',\n",
       "  'live'],\n",
       " ['score',\n",
       "  'result',\n",
       "  'determine',\n",
       "  'yield',\n",
       "  'Alfred',\n",
       "  'test',\n",
       "  'measure',\n",
       "  'child',\n",
       "  'researcher',\n",
       "  'population'],\n",
       " ['question',\n",
       "  'time',\n",
       "  'sex',\n",
       "  'language',\n",
       "  'go',\n",
       "  'know',\n",
       "  'desire',\n",
       "  'baby',\n",
       "  'little',\n",
       "  'world'],\n",
       " ['68',\n",
       "  'play',\n",
       "  'game',\n",
       "  'perform',\n",
       "  'child',\n",
       "  'cognitive',\n",
       "  'late',\n",
       "  'ability',\n",
       "  'Music',\n",
       "  'man'],\n",
       " ['dinosaur',\n",
       "  'coating',\n",
       "  'go',\n",
       "  'god',\n",
       "  'year',\n",
       "  'life',\n",
       "  'see',\n",
       "  'time',\n",
       "  'struggle',\n",
       "  'light'],\n",
       " ['price',\n",
       "  'motor',\n",
       "  'suddenly',\n",
       "  'search',\n",
       "  'laughter',\n",
       "  'nice',\n",
       "  'maybe',\n",
       "  'dollar',\n",
       "  '3,000',\n",
       "  'yes'],\n",
       " ['weapon',\n",
       "  '3,000',\n",
       "  'urchin',\n",
       "  'wo',\n",
       "  'pull',\n",
       "  'mountain',\n",
       "  'item',\n",
       "  'tomorrow',\n",
       "  'dollar',\n",
       "  'large'],\n",
       " ['♫',\n",
       "  'mosquito',\n",
       "  'like',\n",
       "  'furnace',\n",
       "  'diamond',\n",
       "  'river',\n",
       "  'black',\n",
       "  'revive',\n",
       "  'Blue',\n",
       "  'hustle'],\n",
       " ['underlying',\n",
       "  'car',\n",
       "  'drug',\n",
       "  'test',\n",
       "  'measure',\n",
       "  'stop',\n",
       "  'drive',\n",
       "  'dollar',\n",
       "  'want',\n",
       "  'like'],\n",
       " ['liquid',\n",
       "  'like',\n",
       "  'argue',\n",
       "  'sugar',\n",
       "  'year',\n",
       "  'organism',\n",
       "  'think',\n",
       "  'thing',\n",
       "  'life',\n",
       "  'material'],\n",
       " ['bike',\n",
       "  'head',\n",
       "  'mountain',\n",
       "  'buy',\n",
       "  'day',\n",
       "  'maybe',\n",
       "  'find',\n",
       "  'little',\n",
       "  'laughter',\n",
       "  'probably'],\n",
       " ['like',\n",
       "  'family',\n",
       "  'witness',\n",
       "  'tell',\n",
       "  'civil',\n",
       "  'high',\n",
       "  'school',\n",
       "  'life',\n",
       "  'substance',\n",
       "  'year'],\n",
       " ['cell',\n",
       "  'bone',\n",
       "  'fisherman',\n",
       "  'year',\n",
       "  'work',\n",
       "  'fish',\n",
       "  'like',\n",
       "  'world',\n",
       "  'hypothesis',\n",
       "  'long'],\n",
       " ['think',\n",
       "  'flood',\n",
       "  'cloud',\n",
       "  'year',\n",
       "  'search',\n",
       "  'insect',\n",
       "  'day',\n",
       "  'trap',\n",
       "  'Dr.',\n",
       "  'people'],\n",
       " ['18th',\n",
       "  'County',\n",
       "  'truth',\n",
       "  'rest',\n",
       "  'return',\n",
       "  'work',\n",
       "  'far',\n",
       "  'state',\n",
       "  'pursue',\n",
       "  'play'],\n",
       " ['button',\n",
       "  'think',\n",
       "  'mathematic',\n",
       "  'like',\n",
       "  'go',\n",
       "  'algebra',\n",
       "  'illusion',\n",
       "  'pile',\n",
       "  'change',\n",
       "  'DIY'],\n",
       " ['people',\n",
       "  'carbon',\n",
       "  'Britain',\n",
       "  'climate',\n",
       "  'world',\n",
       "  'level',\n",
       "  'drug',\n",
       "  'Giant',\n",
       "  'start',\n",
       "  'come'],\n",
       " ['like',\n",
       "  '♫',\n",
       "  'election',\n",
       "  'go',\n",
       "  'vote',\n",
       "  'work',\n",
       "  'year',\n",
       "  'oh',\n",
       "  'think',\n",
       "  'laughter'],\n",
       " ['like',\n",
       "  'talk',\n",
       "  'day',\n",
       "  'sexual',\n",
       "  'ear',\n",
       "  'go',\n",
       "  'little',\n",
       "  'think',\n",
       "  'thing',\n",
       "  'time'],\n",
       " ['Peter',\n",
       "  'spring',\n",
       "  'power',\n",
       "  'hold',\n",
       "  'system',\n",
       "  'care',\n",
       "  'finally',\n",
       "  'laughter',\n",
       "  'experience',\n",
       "  'far'],\n",
       " ['relative',\n",
       "  'working',\n",
       "  'reflect',\n",
       "  '100',\n",
       "  'ability',\n",
       "  'group',\n",
       "  'design',\n",
       "  'memory',\n",
       "  'age',\n",
       "  'like'],\n",
       " ['work',\n",
       "  'like',\n",
       "  'year',\n",
       "  'time',\n",
       "  'Africa',\n",
       "  'laughter',\n",
       "  'fill',\n",
       "  'way',\n",
       "  'people',\n",
       "  'look'],\n",
       " ['people',\n",
       "  'year',\n",
       "  'go',\n",
       "  'world',\n",
       "  'life',\n",
       "  'planet',\n",
       "  'like',\n",
       "  'thing',\n",
       "  'laughter',\n",
       "  'think'],\n",
       " ['reasoning',\n",
       "  'adjust',\n",
       "  'ability',\n",
       "  'skill',\n",
       "  'reflect',\n",
       "  'test',\n",
       "  'point',\n",
       "  'method',\n",
       "  'result',\n",
       "  'age'],\n",
       " ['Truth',\n",
       "  'return',\n",
       "  'system',\n",
       "  'force',\n",
       "  'fight',\n",
       "  'justice',\n",
       "  'hold',\n",
       "  'life',\n",
       "  'century',\n",
       "  'money'],\n",
       " ['air',\n",
       "  'ancient',\n",
       "  'sleep',\n",
       "  'plant',\n",
       "  'feather',\n",
       "  'go',\n",
       "  'day',\n",
       "  'oxygen',\n",
       "  'mammal',\n",
       "  'year'],\n",
       " ['seller',\n",
       "  'officer',\n",
       "  'money',\n",
       "  'laughter',\n",
       "  'deal',\n",
       "  'nice',\n",
       "  'drive',\n",
       "  'file',\n",
       "  'find',\n",
       "  'people'],\n",
       " ['behavioral',\n",
       "  'widely',\n",
       "  'think',\n",
       "  'tell',\n",
       "  'feel',\n",
       "  'like',\n",
       "  'thank',\n",
       "  'country',\n",
       "  'mature',\n",
       "  'involve'],\n",
       " ['know',\n",
       "  'go',\n",
       "  'time',\n",
       "  'year',\n",
       "  'think',\n",
       "  'people',\n",
       "  'like',\n",
       "  'heart',\n",
       "  'talk',\n",
       "  'tell'],\n",
       " ['year',\n",
       "  'people',\n",
       "  'go',\n",
       "  'like',\n",
       "  'say',\n",
       "  'work',\n",
       "  'life',\n",
       "  'belong',\n",
       "  'know',\n",
       "  'think'],\n",
       " ['thing',\n",
       "  'think',\n",
       "  'go',\n",
       "  'know',\n",
       "  'laughter',\n",
       "  'tell',\n",
       "  'come',\n",
       "  'way',\n",
       "  'want',\n",
       "  'sleep'],\n",
       " ['marry', 'go', 'day', 'like', 'datum', 'boss', 'new', 'want', 'ask', 'come'],\n",
       " ['know',\n",
       "  'organ',\n",
       "  'fluid',\n",
       "  'soil',\n",
       "  'want',\n",
       "  'think',\n",
       "  'human',\n",
       "  'man',\n",
       "  'people',\n",
       "  'body'],\n",
       " ['number',\n",
       "  'book',\n",
       "  'generation',\n",
       "  'friction',\n",
       "  'infinite',\n",
       "  'variable',\n",
       "  'line',\n",
       "  'arthritis',\n",
       "  'Audio',\n",
       "  'swipe'],\n",
       " ['Picture',\n",
       "  'declare',\n",
       "  'local',\n",
       "  'road',\n",
       "  'large',\n",
       "  'yes',\n",
       "  'dollar',\n",
       "  'buy',\n",
       "  'go',\n",
       "  'laughter'],\n",
       " ['go',\n",
       "  'know',\n",
       "  'like',\n",
       "  'time',\n",
       "  'think',\n",
       "  'thing',\n",
       "  'school',\n",
       "  'way',\n",
       "  'say',\n",
       "  'year'],\n",
       " ['like',\n",
       "  'people',\n",
       "  'laughter',\n",
       "  'kind',\n",
       "  'plastic',\n",
       "  'thing',\n",
       "  'think',\n",
       "  'try',\n",
       "  'get',\n",
       "  'go'],\n",
       " ['think',\n",
       "  'say',\n",
       "  'like',\n",
       "  'swim',\n",
       "  'feel',\n",
       "  'laughter',\n",
       "  'thing',\n",
       "  'know',\n",
       "  'year',\n",
       "  'joke'],\n",
       " ['know',\n",
       "  'like',\n",
       "  'life',\n",
       "  'world',\n",
       "  'year',\n",
       "  'people',\n",
       "  'live',\n",
       "  'go',\n",
       "  'find',\n",
       "  'time'],\n",
       " ['fish',\n",
       "  'year',\n",
       "  'river',\n",
       "  'old',\n",
       "  'immigration',\n",
       "  'like',\n",
       "  'memory',\n",
       "  'go',\n",
       "  'democracy',\n",
       "  'young'],\n",
       " ['IQ',\n",
       "  'design',\n",
       "  'factor',\n",
       "  '100',\n",
       "  'represent',\n",
       "  'visual',\n",
       "  'point',\n",
       "  'test',\n",
       "  'perform',\n",
       "  'population'],\n",
       " ['toothpaste',\n",
       "  'humble',\n",
       "  'like',\n",
       "  'look',\n",
       "  'actually',\n",
       "  'run',\n",
       "  'way',\n",
       "  'foot',\n",
       "  'love',\n",
       "  'know'],\n",
       " ['characterize',\n",
       "  'mouth',\n",
       "  'give',\n",
       "  'way',\n",
       "  'fact',\n",
       "  'like',\n",
       "  'look',\n",
       "  'laughter',\n",
       "  'year',\n",
       "  'run']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74ea5745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from octis.models.NMF import NMF\n",
    "\n",
    "\n",
    "nmf = NMF(random_state=RANDOM_STATE) \n",
    "nmf_model = nmf.train_model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51628f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['come', 'human', 'laughter', 'system', 'different', 'hand', 'world', 'sleep', 'idea', 'water'], ['planet', 'girl', 'dog', 'design', 'star', 'light', 'think', 'Earth', 'work', 'boy'], ['♪', 'blah', 'new', 'behavior', 'music', 'light', 'animal', 'oh', 'night', 'girl'], ['♫', 'want', 'know', 'thing', 'world', 'look', 'everybody', 'time', 'get', 'change'], ['year', 'like', 'billion', 'planet', 'plant', 'come', 'minute', 'understand', 'New', 'life'], ['♪', 'come', 'information', 'call', 'fact', 'fly', 'mammal', 'time', 'track', 'second'], ['think', 'like', 'space', 'design', 'way', 'talk', 'come', 'sort', 'mean', 'happiness'], ['planet', 'phone', 'technology', 'mobile', 'live', 'power', 'future', 'Earth', 'wave', 'universe'], ['world', 'live', 'country', 'want', 'China', 'journey', 'long', 'story', 'map', 'build'], ['brain', 'da', 'yeah', 'body', '♪', 'want', 'tell', 'human', 'say', 'hand'], ['people', 'city', 'today', 'million', 'number', 'insect', 'create', 'specie', 'right', 'take'], ['world', 'go', 'baby', 'way', 'play', 'start', 'kind', 'mockingbird', 'hear', 'stunt'], ['want', 'kid', 'love', 'picture', 'laughter', 'word', 'age', 'read', 'old', 'student'], ['design', 'good', 'actually', 'computer', 'build', 'fast', 'make', 'designer', 'small', 'robot'], ['day', 'big', 'new', 'feel', 'single', 'memory', 'building', 'walk', 'person', 'start'], ['child', 'people', 'year', 'like', 'family', 'feel', 'start', 'book', 'help', 'car'], ['know', 'talk', 'come', 'tell', 'like', 'plant', 'say', 'get', 'well', 'kind'], ['life', 'think', 'future', 'little', 'talk', 'different', 'bit', 'Earth', 'self', 'try'], ['build', 'work', 'feel', 'life', 'energy', 'stuff', 'big', 'time', 'information', 'ask'], ['think', 'tell', 'get', 'way', 'feel', 'sleep', 'lot', 'gap', 'go', 'Derek'], ['yeah', 'learn', 'thing', 'think', 'na', 'lion', '10', 'hour', 'different', 'die'], ['story', 'brain', 'tell', 'read', 'turn', 'get', 'disorder', 'understand', 'economic', 'reason'], ['know', 'laughter', 'work', 'ask', 'year', 'company', 'say', 'world', 'believe', 'start'], ['go', 'like', 'get', 'come', 'house', 'laughter', 'man', 'school', 'room', 'hand'], ['come', 'bad', 'marry', 'look', 'like', 'write', 'see', 'want', 'ago', 'good'], ['♫', 'like', 'little', 'people', 'big', 'Blue', 'da', 'soon', 'question', 'actually'], ['get', 'Einstein', 'ask', 'SW', 'come', 'laughter', 'hear', 'e', 'question', 'power'], ['thing', 'get', 'grow', 'book', 'live', 'play', 'find', 'learn', 'happen', 'different'], ['work', 'book', 'New', 'like', 'York', 'call', 'world', 'come', 'thing', 'map'], ['problem', 'family', 'people', 'live', 'HIV', 'solve', 'face', 'need', 'big', 'way'], ['think', 'thing', 'know', 'kind', 'laughter', 'people', 'today', 'try', 'important', 'go'], ['woman', 'come', 'universe', 'run', 'health', 'time', 'laughter', 'lie', 'technology', 'machine'], ['city', 'year', 'ago', 'get', 'world', 'thing', 'day', 'good', 'computer', 'girl'], ['people', 'laughter', 'world', 'thing', 'think', 'new', 'idea', 'science', 'lot', 'mind'], ['tell', 'story', 'live', 'year', '♪', 'laughter', 'family', 'come', 'school', 'talk'], ['old', 'good', 'real', 'work', 'run', 'live', 'question', 'soon', 'touch', 'generation'], ['percent', 'happen', 'brain', 'ocean', 'oil', 'look', 'try', 'Africa', 'disease', 'information'], ['world', 'change', 'country', 'look', 'big', 'idea', 'today', 'government', 'universe', 'wish'], ['world', 'time', 'people', 'start', 'million', 'want', 'plant', 'system', 'fact', 'car'], ['people', 'talk', 'go', 'laughter', 'die', 'love', 'know', 'find', 'live', 'year'], ['number', 'talk', 'life', 'year', 'start', 'little', 'Earth', 'bit', 'fire', 'kind'], ['life', 'work', 'art', 'bring', 'need', 'world', 'car', 'eye', 'thing', 'fact'], ['well', 'new', 'want', 'man', 'different', 'way', 'say', 'help', 'use', 'brain'], ['actually', 'sound', 'try', 'way', 'idea', 'room', 'computer', 'ask', 'write', 'question'], ['go', 'Africa', 'know', 'Music', 'number', 'high', 'thank', 'call', 'come', 'sound'], ['look', 'think', 'let', 'human', 'ocean', 'face', 'image', 'little', 'number', 'start'], ['word', 'computer', 'cancer', 'try', 'right', 'education', 'black', 'come', 'learn', 'know'], ['brain', 'change', 'year', 'like', 'people', 'world', 'life', 'human', 'great', 'idea'], ['find', 'heart', 'think', 'dream', 'name', 'feel', 'information', 'coral', 'street', 'young'], ['like', 'feel', 'day', 'time', 'kind', 'know', 'parent', 'little', 'laughter', 'lot'], ['woman', 'go', 'say', 'talk', 'need', 'people', 'day', 'see', 'sort', 'call'], ['way', 'place', 'start', 'think', 'today', 'little', 'build', 'America', 'value', 'machine'], ['go', 'say', 'good', 'little', 'tell', 'know', 'world', 'talk', 'right', 'great'], ['go', 'student', 'record', 'black', 'want', 'level', 'television', 'different', 'language', 'change'], ['need', 'laughter', 'energy', 'da', 'power', 'call', 'great', 'good', 'time', 'start'], ['people', 'call', 'place', 'animal', 'get', 'heart', 'young', 'pain', 'war', 'thousand'], ['love', 'year', 'old', 'thing', 'datum', 'rocket', 'video', 'star', 'month', 'make'], ['thing', 'water', 'world', 'go', 'like', 'technology', 'actually', 'people', 'happen', 'year'], ['live', 'kid', 'child', 'Music', 'number', 'animal', 'lion', 'Applause', 'change', 'idea'], ['time', 'word', 'world', 'economy', 'growth', 'like', 'need', 'class', 'open', 'coral'], ['know', 'school', 'Earth', 'little', 'way', 'want', 'planet', 'book', 'truth', 'water'], ['problem', 'man', 'go', 'find', 'day', 'think', 'try', 'laughter', 'cancer', 'start'], ['♪', 'people', 'think', 'art', 'year', 'artist', 'kid', 'day', 'story', 'lot'], ['want', 'people', 'find', 'talk', 'thing', 'think', 'like', 'time', 'work', 'game'], ['war', 'tell', 'year', 'call', 'end', 'hold', 'answer', 'great', 'good', 'game'], ['tell', 'story', 'like', 'thing', 'people', 'way', 'start', 'change', 'computer', 'go'], ['question', 'way', 'change', 'paper', 'find', 'answer', 'climate', 'cell', 'ask', 'animal'], ['go', 'car', 'actually', 'space', 'come', 'drive', 'take', 'year', 'tell', 'use'], ['black', 'hole', 'galaxy', 'universe', 'find', 'star', 'space', 'small', 'object', 'story'], ['question', 'go', 'ask', 'think', 'try', 'answer', 'get', 'start', 'love', 'watch'], ['laughter', 'say', 'actually', 'body', 'call', 'year', 'work', 'war', 'science', 'sense'], ['year', 'day', 'know', 'million', 'woman', 'life', 'human', 'blah', 'ago', 'story'], ['robot', 'people', 'work', 'power', 'want', 'year', 'oil', 'build', 'body', 'create'], ['choice', 'water', 'think', 'live', 'hard', 'say', 'ocean', 'mammal', 'see', 'sense'], ['cancer', 'city', 'idea', '♪', 'people', 'like', 'music', 'disease', 'percent', 'million'], ['laughter', 'people', 'speak', 'language', 'find', 'think', 'number', 'ask', 'patient', 'friend'], ['year', 'time', 'ago', 'today', 'country', 'child', 'disease', 'way', 'room', 'human'], ['think', 'technology', 'end', 'happen', 'open', 'actually', 'world', 'new', 'small', 'datum'], ['right', 'know', 'talk', 'thing', 'happiness', 'happy', 'question', 'drug', 'Apple', 'turn'], ['go', 'lot', 'need', 'people', 'thing', 'happen', 'come', 'story', 'relationship', 'interested'], ['man', 'life', 'far', 'new', 'hole', 'death', 'black', 'cell', 'home', 'wall'], ['technology', 'China', 'country', 'speak', 'smell', 'music', 'specie', 'mosquito', 'call', 'India'], ['time', 'go', 'look', 'love', 'day', 'change', 'space', 'come', 'old', 'turn'], ['brain', 'way', 'use', 'girl', 'talk', 'matter', 'kind', 'lion', 'male', 'boy'], ['cell', 'body', 'human', 'year', 'woman', 'color', 'like', 'design', 'history', 'look'], ['man', 'game', 'play', 'child', 'love', 'time', 'grow', 'tm', 'woman', 'father'], ['like', 'right', 'laughter', 'know', 'look', 'let', 'light', 'form', 'lot', 'turn'], ['know', 'time', 'woman', 'live', 'music', 'water', 'day', 'good', 'young', 'Music'], ['right', 'school', '♫', 'use', 'start', 'way', 'come', 'well', 'good', 'da'], ['go', 'thing', '♪', 'TED', 'good', 'talk', 'feel', 'baby', 'actually', 'think'], ['say', 'thing', 'find', 'love', 'look', 'world', 'life', 'child', 'tell', 'planet'], ['grow', 'work', 'cell', 'go', 'woman', 'day', 'new', 'little', 'baby', 'girl'], ['year', 'go', 'old', 'look', 'well', 'mean', 'actually', 'start', 'lot', 'picture'], ['human', 'thing', 'tell', 'science', 'good', 'people', 'value', 'language', 'like', 'job'], ['time', 'change', 'like', 'sleep', 'hear', 'tell', 'life', 'sound', 'way', 'story'], ['♫', 'think', 'like', 'go', 'year', 'old', 'phone', 'man', 'black', 'let'], ['work', 'talk', 'today', 'year', 'learn', 'woman', 'start', 'write', 'want', 'let'], ['year', 'new', 'know', 'car', 'remember', 'lot', 'billion', 'learn', 'computer', 'program'], ['♫', 'da', 'get', 'kind', 'father', 'heh', 'story', 'Blue', 'tree', 'river'], ['think', 'get', 'right', 'idea', 'take', 'time', 'hand', 'great', 'thing', 'world']]\n"
     ]
    }
   ],
   "source": [
    "print(nmf_model['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98333ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Coherence Score: 0.5065803754895994\n",
      "NMF Coherence Score: 0.4873868343908957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA U-Mass Score: nan\n",
      "NMF U-Mass Score: -2.2397111666826803\n"
     ]
    }
   ],
   "source": [
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "\n",
    "coherence_model = Coherence(measure=\"c_v\")\n",
    "lda_coherence = coherence_model.score(lda_model)\n",
    "print(f\"LDA Coherence Score: {lda_coherence}\")\n",
    "\n",
    "\n",
    "nmf_coherence = coherence_model.score(nmf_model)\n",
    "print(f\"NMF Coherence Score: {nmf_coherence}\")\n",
    "\n",
    "\n",
    "umass_model = Coherence(measure=\"u_mass\")\n",
    "lda_coherence = umass_model.score(lda_model)\n",
    "print(f\"LDA U-Mass Score: {lda_coherence}\")\n",
    "\n",
    "nmf_coherence = umass_model.score(nmf_model)\n",
    "print(f\"NMF U-Mass Score: {nmf_coherence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9f21854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current call:  0\n",
      "Current call:  1\n",
      "Current call:  2\n",
      "Current call:  3\n",
      "Current call:  4\n",
      "Current call:  5\n",
      "Current call:  6\n",
      "Current call:  7\n",
      "Current call:  8\n",
      "Current call:  9\n",
      "Current call:  10\n",
      "Current call:  11\n",
      "Current call:  12\n",
      "Current call:  13\n",
      "Current call:  14\n",
      "Current call:  15\n",
      "Current call:  16\n",
      "Current call:  17\n",
      "Current call:  18\n",
      "Current call:  19\n"
     ]
    }
   ],
   "source": [
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Integer\n",
    "\n",
    "search_space = {\"num_topics\": Integer(5, 20),\n",
    "                \"alpha\": Real(0.01, 1.0, \"log-uniform\"),\n",
    "                \"eta\": Real(0.01, 1.0, \"log-uniform\"),}\n",
    "\n",
    "optimizer = Optimizer()\n",
    "eval_metrics = Coherence(texts=dataset.get_corpus(), measure=\"c_v\")\n",
    "\n",
    "if not os.path.exists('results'):   \n",
    "    os.makedirs('results')\n",
    "    \n",
    "optimize_result = optimizer.optimize(LDA(random_state=RANDOM_STATE), \n",
    "                                     dataset, \n",
    "                                     eval_metrics, \n",
    "                                     search_space, \n",
    "                                    save_path='results',\n",
    "                                    number_of_call=20,\n",
    "                                    model_runs=5)\n",
    "optimize_result.save_to_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08833d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current call:  0\n",
      "Current call:  1\n",
      "Current call:  2\n",
      "Current call:  3\n",
      "Current call:  4\n",
      "Current call:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [11] before, using random point [19]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current call:  6\n",
      "Current call:  7\n",
      "Current call:  8\n",
      "Current call:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [13] before, using random point [8]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Integer\n",
    "\n",
    "search_space = {\"num_topics\": Integer(3, 20)}\n",
    "optimizer = Optimizer()\n",
    "eval_metrics = Coherence(texts=dataset.get_corpus(), measure=\"c_v\")\n",
    "\n",
    "if not os.path.exists('results'):   \n",
    "    os.makedirs('results')\n",
    "    \n",
    "optimize_result = optimizer.optimize(NMF(random_state=RANDOM_STATE), \n",
    "                                     dataset, \n",
    "                                     eval_metrics, \n",
    "                                     search_space, \n",
    "                                    save_path='results',\n",
    "                                    number_of_call=10,\n",
    "                                    model_runs=3)\n",
    "optimize_result.save_to_csv(\"nmf_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0ef7785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current call:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1320: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=24853, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=11, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=24853, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=11, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=11, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.12 .. Rec_loss: 657.52 .. NELBO: 657.64\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.08 .. Rec_loss: 647.68 .. NELBO: 647.76\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.03 .. Rec_loss: 561.74 .. NELBO: 561.77\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 580.7 .. NELBO: 580.72\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 560.47 .. NELBO: 560.47\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 579.11 .. NELBO: 579.11\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 560.2 .. NELBO: 560.2\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 578.72 .. NELBO: 578.72\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.89 .. NELBO: 559.89\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 578.38 .. NELBO: 578.38\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.61 .. NELBO: 559.61\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 578.12 .. NELBO: 578.12\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.39 .. NELBO: 559.39\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.91 .. NELBO: 577.91\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.21 .. NELBO: 559.21\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.74 .. NELBO: 577.74\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.08 .. NELBO: 559.08\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.63 .. NELBO: 577.63\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.94 .. NELBO: 558.94\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.5 .. NELBO: 577.5\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.88 .. NELBO: 558.88\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.43 .. NELBO: 577.43\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.8 .. NELBO: 558.8\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.36 .. NELBO: 577.36\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.72 .. NELBO: 558.72\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.29 .. NELBO: 577.29\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.68 .. NELBO: 558.68\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.26 .. NELBO: 577.26\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.65 .. NELBO: 558.65\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.21 .. NELBO: 577.21\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.62 .. NELBO: 558.62\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.2 .. NELBO: 577.2\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.59 .. NELBO: 558.59\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.16 .. NELBO: 577.16\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.55 .. NELBO: 558.55\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.13 .. NELBO: 577.13\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.51 .. NELBO: 558.51\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.11 .. NELBO: 577.11\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.5 .. NELBO: 558.5\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.1 .. NELBO: 577.1\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.52 .. NELBO: 558.52\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.1 .. NELBO: 577.1\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.51 .. NELBO: 558.51\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.1 .. NELBO: 577.1\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.48 .. NELBO: 558.48\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.07 .. NELBO: 577.07\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.45 .. NELBO: 558.45\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.04 .. NELBO: 577.04\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.43 .. NELBO: 558.43\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.01 .. NELBO: 577.01\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.41 .. NELBO: 558.41\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.0 .. NELBO: 577.0\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.4 .. NELBO: 558.4\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.0 .. NELBO: 577.0\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.35 .. NELBO: 558.35\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.96 .. NELBO: 576.96\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.35 .. NELBO: 558.35\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.95 .. NELBO: 576.95\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.36 .. NELBO: 558.36\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.96 .. NELBO: 576.96\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.37 .. NELBO: 558.37\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.96 .. NELBO: 576.96\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.39 .. NELBO: 558.39\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.97 .. NELBO: 576.97\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.35 .. NELBO: 558.35\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.93 .. NELBO: 576.93\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.29 .. NELBO: 558.29\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.9 .. NELBO: 576.9\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.3 .. NELBO: 558.3\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.89 .. NELBO: 576.89\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.29 .. NELBO: 558.29\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.88 .. NELBO: 576.88\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.27 .. NELBO: 558.27\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.85 .. NELBO: 576.85\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.29 .. NELBO: 558.29\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.87 .. NELBO: 576.87\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.28 .. NELBO: 558.28\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.86 .. NELBO: 576.86\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.28 .. NELBO: 558.28\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.87 .. NELBO: 576.87\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.31 .. NELBO: 558.31\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.88 .. NELBO: 576.88\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.29 .. NELBO: 558.29\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.85 .. NELBO: 576.85\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.27 .. NELBO: 558.27\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.84 .. NELBO: 576.84\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.25 .. NELBO: 558.25\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.83 .. NELBO: 576.83\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.25 .. NELBO: 558.25\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.82 .. NELBO: 576.82\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.23 .. NELBO: 558.23\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.8 .. NELBO: 576.8\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.23 .. NELBO: 558.23\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.83 .. NELBO: 576.83\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.27 .. NELBO: 558.27\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.83 .. NELBO: 576.83\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.3 .. NELBO: 558.3\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.83 .. NELBO: 576.83\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.26 .. NELBO: 558.26\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.85 .. NELBO: 576.85\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.25 .. NELBO: 558.25\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.78 .. NELBO: 576.78\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.22 .. NELBO: 558.22\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.76 .. NELBO: 576.76\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.21 .. NELBO: 558.21\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.79 .. NELBO: 576.79\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.2 .. NELBO: 558.2\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.73 .. NELBO: 576.73\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.22 .. NELBO: 558.22\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.75 .. NELBO: 576.75\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.18 .. NELBO: 558.18\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.75 .. NELBO: 576.75\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.14 .. NELBO: 558.14\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.69 .. NELBO: 576.69\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.18 .. NELBO: 558.18\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.7 .. NELBO: 576.7\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.15 .. NELBO: 558.15\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.7 .. NELBO: 576.7\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.12 .. NELBO: 558.12\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.67 .. NELBO: 576.67\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.11 .. NELBO: 558.11\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.67 .. NELBO: 576.67\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.14 .. NELBO: 558.14\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.67 .. NELBO: 576.67\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.15 .. NELBO: 558.15\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.67 .. NELBO: 576.67\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.13 .. NELBO: 558.13\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.66 .. NELBO: 576.66\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.16 .. NELBO: 558.16\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.69 .. NELBO: 576.69\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.16 .. NELBO: 558.16\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.67 .. NELBO: 576.67\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.17 .. NELBO: 558.17\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.68 .. NELBO: 576.68\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.19 .. NELBO: 558.19\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.71 .. NELBO: 576.71\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.18 .. NELBO: 558.18\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.69 .. NELBO: 576.69\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.19 .. NELBO: 558.19\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.71 .. NELBO: 576.71\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.23 .. NELBO: 558.23\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.73 .. NELBO: 576.73\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.19 .. NELBO: 558.19\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.67 .. NELBO: 576.67\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.16 .. NELBO: 558.16\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.68 .. NELBO: 576.68\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.08 .. NELBO: 558.08\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.63 .. NELBO: 576.63\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.11 .. NELBO: 558.11\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.61 .. NELBO: 576.61\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.1 .. NELBO: 558.1\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.59 .. NELBO: 576.59\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.06 .. NELBO: 558.06\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.58 .. NELBO: 576.58\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.02 .. NELBO: 558.02\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.54 .. NELBO: 576.54\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.01 .. NELBO: 558.01\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.52 .. NELBO: 576.52\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.0 .. NELBO: 558.0\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.52 .. NELBO: 576.52\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.99 .. NELBO: 557.99\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.49 .. NELBO: 576.49\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.99 .. NELBO: 557.99\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.5 .. NELBO: 576.5\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.98 .. NELBO: 557.98\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.49 .. NELBO: 576.49\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.0 .. NELBO: 558.0\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.49 .. NELBO: 576.49\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.04 .. NELBO: 558.04\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.52 .. NELBO: 576.52\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.04 .. NELBO: 558.04\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.54 .. NELBO: 576.54\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.01 .. NELBO: 558.01\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.51 .. NELBO: 576.51\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.05 .. NELBO: 558.05\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.55 .. NELBO: 576.55\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.07 .. NELBO: 558.07\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.57 .. NELBO: 576.57\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.11 .. NELBO: 558.11\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.56 .. NELBO: 576.56\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.14 .. NELBO: 558.14\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.58 .. NELBO: 576.58\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.07 .. NELBO: 558.07\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.57 .. NELBO: 576.57\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.01 .. NELBO: 558.01\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.53 .. NELBO: 576.53\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.03 .. NELBO: 558.03\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.51 .. NELBO: 576.51\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 558.03 .. NELBO: 558.04\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.48 .. NELBO: 576.48\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.99 .. NELBO: 557.99\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.47 .. NELBO: 576.47\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.89 .. NELBO: 557.89\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.38 .. NELBO: 576.38\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.89 .. NELBO: 557.89\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.38 .. NELBO: 576.38\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.85 .. NELBO: 557.85\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.34 .. NELBO: 576.34\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.83 .. NELBO: 557.83\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.31 .. NELBO: 576.31\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1320: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=24853, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=11, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=24853, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=11, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=11, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.09 .. Rec_loss: 658.32 .. NELBO: 658.41\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 648.13 .. NELBO: 648.2\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.07 .. Rec_loss: 561.72 .. NELBO: 561.79\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.05 .. Rec_loss: 580.7 .. NELBO: 580.75\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 560.53 .. NELBO: 560.54\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 579.15 .. NELBO: 579.16\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 560.24 .. NELBO: 560.24\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 578.75 .. NELBO: 578.75\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.9 .. NELBO: 559.9\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 578.39 .. NELBO: 578.39\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.61 .. NELBO: 559.61\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 578.12 .. NELBO: 578.12\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.4 .. NELBO: 559.4\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.91 .. NELBO: 577.91\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.22 .. NELBO: 559.22\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.75 .. NELBO: 577.75\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.08 .. NELBO: 559.08\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.61 .. NELBO: 577.61\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.95 .. NELBO: 558.95\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.5 .. NELBO: 577.5\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.87 .. NELBO: 558.87\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.42 .. NELBO: 577.42\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.79 .. NELBO: 558.79\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.35 .. NELBO: 577.35\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.74 .. NELBO: 558.74\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.3 .. NELBO: 577.3\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.7 .. NELBO: 558.7\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.25 .. NELBO: 577.25\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.7 .. NELBO: 558.7\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.25 .. NELBO: 577.25\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.66 .. NELBO: 558.66\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.23 .. NELBO: 577.23\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.57 .. NELBO: 558.57\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.15 .. NELBO: 577.15\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.56 .. NELBO: 558.56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m):   \n\u001b[1;32m     10\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m optimize_result \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mETM\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_partitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43meval_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43msearch_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mnumber_of_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmodel_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimize_result\u001b[38;5;241m.\u001b[39msave_to_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metm_result.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/octis/optimization/optimizer.py:184\u001b[0m, in \u001b[0;36mOptimizer.optimize\u001b[0;34m(self, model, dataset, metric, search_space, extra_metrics, number_of_call, n_random_starts, initial_point_generator, optimization_type, model_runs, surrogate_model, kernel, acq_func, random_state, x0, y0, save_models, save_step, save_name, save_path, early_stop, early_step, plot_best_seen, plot_model, plot_name, log_scale_plot, topk)\u001b[0m\n\u001b[1;32m    181\u001b[0m opt \u001b[38;5;241m=\u001b[39m choose_optimizer(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Perform Bayesian Optimization\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimization_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/octis/optimization/optimizer.py:323\u001b[0m, in \u001b[0;36mOptimizer._optimization_loop\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    322\u001b[0m     next_x \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mask()\n\u001b[0;32m--> 323\u001b[0m     f_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_objective_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# Update the opt using (next_x,f_val)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m res \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mtell(next_x, f_val)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/octis/optimization/optimizer.py:241\u001b[0m, in \u001b[0;36mOptimizer._objective_function\u001b[0;34m(self, hyperparameter_values)\u001b[0m\n\u001b[1;32m    235\u001b[0m different_model_runs_extra_metrics \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_metrics))]\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_runs):\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# Prepare model\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# Score of the model\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\u001b[38;5;241m.\u001b[39mscore(model_output)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/octis/models/ETM.py:93\u001b[0m, in \u001b[0;36mETM.train_model\u001b[0;34m(self, dataset, hyperparameters, top_words, op_path)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[1;32m     90\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, path\u001b[38;5;241m=\u001b[39mop_path)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 93\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/octis/models/ETM.py:180\u001b[0m, in \u001b[0;36mETM._train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    178\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m    179\u001b[0m                                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m acc_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(recon_loss)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    183\u001b[0m acc_kl_theta_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(kld_theta)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/torch/optim/adam.py:398\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    395\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_decay \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 398\u001b[0m     grad \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(param):\n\u001b[1;32m    401\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(grad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from octis.optimization.optimizer import Optimizer\n",
    "from skopt.space.space import Real, Integer\n",
    "from octis.models.ETM import ETM\n",
    "\n",
    "search_space = {\"num_topics\": Integer(3, 20)}\n",
    "optimizer = Optimizer()\n",
    "eval_metrics = Coherence(texts=dataset.get_corpus(), measure=\"c_v\")\n",
    "\n",
    "if not os.path.exists('results'):   \n",
    "    os.makedirs('results')\n",
    "    \n",
    "optimize_result = optimizer.optimize(ETM(use_partitions=False), \n",
    "                                     dataset, \n",
    "                                     eval_metrics, \n",
    "                                     search_space, \n",
    "                                    save_path='results',\n",
    "                                    number_of_call=10,\n",
    "                                    model_runs=3)\n",
    "optimize_result.save_to_csv(\"etm_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed42cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_project/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1320: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: ETM(\n",
      "  (t_drop): Dropout(p=0.5, inplace=False)\n",
      "  (theta_act): ReLU()\n",
      "  (rho): Linear(in_features=300, out_features=24853, bias=False)\n",
      "  (alphas): Linear(in_features=300, out_features=10, bias=False)\n",
      "  (q_theta): Sequential(\n",
      "    (0): Linear(in_features=24853, out_features=800, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (mu_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
      "  (logsigma_q_theta): Linear(in_features=800, out_features=10, bias=True)\n",
      ")\n",
      "Epoch: 1 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.02 .. Rec_loss: 657.33 .. NELBO: 657.35\n",
      "****************************************************************************************************\n",
      "Epoch----->1 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 647.57 .. NELBO: 647.58\n",
      "****************************************************************************************************\n",
      "Epoch: 2 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 561.75 .. NELBO: 561.76\n",
      "****************************************************************************************************\n",
      "Epoch----->2 .. LR: 0.005 .. KL_theta: 0.01 .. Rec_loss: 580.7 .. NELBO: 580.71\n",
      "****************************************************************************************************\n",
      "Epoch: 3 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 560.44 .. NELBO: 560.44\n",
      "****************************************************************************************************\n",
      "Epoch----->3 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 579.07 .. NELBO: 579.07\n",
      "****************************************************************************************************\n",
      "Epoch: 4 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 560.16 .. NELBO: 560.16\n",
      "****************************************************************************************************\n",
      "Epoch----->4 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 578.68 .. NELBO: 578.68\n",
      "****************************************************************************************************\n",
      "Epoch: 5 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.89 .. NELBO: 559.89\n",
      "****************************************************************************************************\n",
      "Epoch----->5 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 578.37 .. NELBO: 578.37\n",
      "****************************************************************************************************\n",
      "Epoch: 6 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.61 .. NELBO: 559.61\n",
      "****************************************************************************************************\n",
      "Epoch----->6 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 578.11 .. NELBO: 578.11\n",
      "****************************************************************************************************\n",
      "Epoch: 7 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.4 .. NELBO: 559.4\n",
      "****************************************************************************************************\n",
      "Epoch----->7 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.91 .. NELBO: 577.91\n",
      "****************************************************************************************************\n",
      "Epoch: 8 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.23 .. NELBO: 559.23\n",
      "****************************************************************************************************\n",
      "Epoch----->8 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.75 .. NELBO: 577.75\n",
      "****************************************************************************************************\n",
      "Epoch: 9 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 559.08 .. NELBO: 559.08\n",
      "****************************************************************************************************\n",
      "Epoch----->9 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.62 .. NELBO: 577.62\n",
      "****************************************************************************************************\n",
      "Epoch: 10 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.99 .. NELBO: 558.99\n",
      "****************************************************************************************************\n",
      "Epoch----->10 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.53 .. NELBO: 577.53\n",
      "****************************************************************************************************\n",
      "Epoch: 11 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.88 .. NELBO: 558.88\n",
      "****************************************************************************************************\n",
      "Epoch----->11 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.44 .. NELBO: 577.44\n",
      "****************************************************************************************************\n",
      "Epoch: 12 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.83 .. NELBO: 558.83\n",
      "****************************************************************************************************\n",
      "Epoch----->12 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.39 .. NELBO: 577.39\n",
      "****************************************************************************************************\n",
      "Epoch: 13 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.74 .. NELBO: 558.74\n",
      "****************************************************************************************************\n",
      "Epoch----->13 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.3 .. NELBO: 577.3\n",
      "****************************************************************************************************\n",
      "Epoch: 14 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.7 .. NELBO: 558.7\n",
      "****************************************************************************************************\n",
      "Epoch----->14 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.26 .. NELBO: 577.26\n",
      "****************************************************************************************************\n",
      "Epoch: 15 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.64 .. NELBO: 558.64\n",
      "****************************************************************************************************\n",
      "Epoch----->15 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.23 .. NELBO: 577.23\n",
      "****************************************************************************************************\n",
      "Epoch: 16 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.64 .. NELBO: 558.64\n",
      "****************************************************************************************************\n",
      "Epoch----->16 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.23 .. NELBO: 577.23\n",
      "****************************************************************************************************\n",
      "Epoch: 17 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.6 .. NELBO: 558.6\n",
      "****************************************************************************************************\n",
      "Epoch----->17 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.18 .. NELBO: 577.18\n",
      "****************************************************************************************************\n",
      "Epoch: 18 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.6 .. NELBO: 558.6\n",
      "****************************************************************************************************\n",
      "Epoch----->18 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.18 .. NELBO: 577.18\n",
      "****************************************************************************************************\n",
      "Epoch: 19 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.58 .. NELBO: 558.58\n",
      "****************************************************************************************************\n",
      "Epoch----->19 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.16 .. NELBO: 577.16\n",
      "****************************************************************************************************\n",
      "Epoch: 20 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.54 .. NELBO: 558.54\n",
      "****************************************************************************************************\n",
      "Epoch----->20 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.13 .. NELBO: 577.13\n",
      "****************************************************************************************************\n",
      "Epoch: 21 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.5 .. NELBO: 558.5\n",
      "****************************************************************************************************\n",
      "Epoch----->21 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.08 .. NELBO: 577.08\n",
      "****************************************************************************************************\n",
      "Epoch: 22 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.48 .. NELBO: 558.48\n",
      "****************************************************************************************************\n",
      "Epoch----->22 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.07 .. NELBO: 577.07\n",
      "****************************************************************************************************\n",
      "Epoch: 23 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.47 .. NELBO: 558.47\n",
      "****************************************************************************************************\n",
      "Epoch----->23 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.06 .. NELBO: 577.06\n",
      "****************************************************************************************************\n",
      "Epoch: 24 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.49 .. NELBO: 558.49\n",
      "****************************************************************************************************\n",
      "Epoch----->24 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.07 .. NELBO: 577.07\n",
      "****************************************************************************************************\n",
      "Epoch: 25 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.49 .. NELBO: 558.49\n",
      "****************************************************************************************************\n",
      "Epoch----->25 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.09 .. NELBO: 577.09\n",
      "****************************************************************************************************\n",
      "Epoch: 26 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.49 .. NELBO: 558.49\n",
      "****************************************************************************************************\n",
      "Epoch----->26 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.09 .. NELBO: 577.09\n",
      "****************************************************************************************************\n",
      "Epoch: 27 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.47 .. NELBO: 558.47\n",
      "****************************************************************************************************\n",
      "Epoch----->27 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 577.06 .. NELBO: 577.06\n",
      "****************************************************************************************************\n",
      "Epoch: 28 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.4 .. NELBO: 558.4\n",
      "****************************************************************************************************\n",
      "Epoch----->28 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.99 .. NELBO: 576.99\n",
      "****************************************************************************************************\n",
      "Epoch: 29 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.35 .. NELBO: 558.35\n",
      "****************************************************************************************************\n",
      "Epoch----->29 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.95 .. NELBO: 576.95\n",
      "****************************************************************************************************\n",
      "Epoch: 30 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.35 .. NELBO: 558.35\n",
      "****************************************************************************************************\n",
      "Epoch----->30 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.96 .. NELBO: 576.96\n",
      "****************************************************************************************************\n",
      "Epoch: 31 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.36 .. NELBO: 558.36\n",
      "****************************************************************************************************\n",
      "Epoch----->31 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.94 .. NELBO: 576.94\n",
      "****************************************************************************************************\n",
      "Epoch: 32 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.36 .. NELBO: 558.36\n",
      "****************************************************************************************************\n",
      "Epoch----->32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.94 .. NELBO: 576.94\n",
      "****************************************************************************************************\n",
      "Epoch: 33 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.32 .. NELBO: 558.32\n",
      "****************************************************************************************************\n",
      "Epoch----->33 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.93 .. NELBO: 576.93\n",
      "****************************************************************************************************\n",
      "Epoch: 34 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.34 .. NELBO: 558.34\n",
      "****************************************************************************************************\n",
      "Epoch----->34 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.93 .. NELBO: 576.93\n",
      "****************************************************************************************************\n",
      "Epoch: 35 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.37 .. NELBO: 558.37\n",
      "****************************************************************************************************\n",
      "Epoch----->35 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.96 .. NELBO: 576.96\n",
      "****************************************************************************************************\n",
      "Epoch: 36 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.35 .. NELBO: 558.35\n",
      "****************************************************************************************************\n",
      "Epoch----->36 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.93 .. NELBO: 576.93\n",
      "****************************************************************************************************\n",
      "Epoch: 37 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.33 .. NELBO: 558.33\n",
      "****************************************************************************************************\n",
      "Epoch----->37 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.91 .. NELBO: 576.91\n",
      "****************************************************************************************************\n",
      "Epoch: 38 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.3 .. NELBO: 558.3\n",
      "****************************************************************************************************\n",
      "Epoch----->38 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.88 .. NELBO: 576.88\n",
      "****************************************************************************************************\n",
      "Epoch: 39 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.25 .. NELBO: 558.25\n",
      "****************************************************************************************************\n",
      "Epoch----->39 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.84 .. NELBO: 576.84\n",
      "****************************************************************************************************\n",
      "Epoch: 40 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.25 .. NELBO: 558.25\n",
      "****************************************************************************************************\n",
      "Epoch----->40 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.85 .. NELBO: 576.85\n",
      "****************************************************************************************************\n",
      "Epoch: 41 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.25 .. NELBO: 558.25\n",
      "****************************************************************************************************\n",
      "Epoch----->41 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.89 .. NELBO: 576.89\n",
      "****************************************************************************************************\n",
      "Epoch: 42 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.32 .. NELBO: 558.32\n",
      "****************************************************************************************************\n",
      "Epoch----->42 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.89 .. NELBO: 576.89\n",
      "****************************************************************************************************\n",
      "Epoch: 43 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.43 .. NELBO: 558.43\n",
      "****************************************************************************************************\n",
      "Epoch----->43 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.97 .. NELBO: 576.97\n",
      "****************************************************************************************************\n",
      "Epoch: 44 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.38 .. NELBO: 558.38\n",
      "****************************************************************************************************\n",
      "Epoch----->44 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.96 .. NELBO: 576.96\n",
      "****************************************************************************************************\n",
      "Epoch: 45 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.26 .. NELBO: 558.26\n",
      "****************************************************************************************************\n",
      "Epoch----->45 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.85 .. NELBO: 576.85\n",
      "****************************************************************************************************\n",
      "Epoch: 46 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.27 .. NELBO: 558.27\n",
      "****************************************************************************************************\n",
      "Epoch----->46 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.84 .. NELBO: 576.84\n",
      "****************************************************************************************************\n",
      "Epoch: 47 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.25 .. NELBO: 558.25\n",
      "****************************************************************************************************\n",
      "Epoch----->47 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.84 .. NELBO: 576.84\n",
      "****************************************************************************************************\n",
      "Epoch: 48 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.21 .. NELBO: 558.21\n",
      "****************************************************************************************************\n",
      "Epoch----->48 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.77 .. NELBO: 576.77\n",
      "****************************************************************************************************\n",
      "Epoch: 49 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.24 .. NELBO: 558.24\n",
      "****************************************************************************************************\n",
      "Epoch----->49 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.81 .. NELBO: 576.81\n",
      "****************************************************************************************************\n",
      "Epoch: 50 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.22 .. NELBO: 558.22\n",
      "****************************************************************************************************\n",
      "Epoch----->50 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.78 .. NELBO: 576.78\n",
      "****************************************************************************************************\n",
      "Epoch: 51 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.15 .. NELBO: 558.15\n",
      "****************************************************************************************************\n",
      "Epoch----->51 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.73 .. NELBO: 576.73\n",
      "****************************************************************************************************\n",
      "Epoch: 52 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.17 .. NELBO: 558.17\n",
      "****************************************************************************************************\n",
      "Epoch----->52 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.76 .. NELBO: 576.76\n",
      "****************************************************************************************************\n",
      "Epoch: 53 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.18 .. NELBO: 558.18\n",
      "****************************************************************************************************\n",
      "Epoch----->53 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.74 .. NELBO: 576.74\n",
      "****************************************************************************************************\n",
      "Epoch: 54 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.21 .. NELBO: 558.21\n",
      "****************************************************************************************************\n",
      "Epoch----->54 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.76 .. NELBO: 576.76\n",
      "****************************************************************************************************\n",
      "Epoch: 55 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.19 .. NELBO: 558.19\n",
      "****************************************************************************************************\n",
      "Epoch----->55 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.76 .. NELBO: 576.76\n",
      "****************************************************************************************************\n",
      "Epoch: 56 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.18 .. NELBO: 558.18\n",
      "****************************************************************************************************\n",
      "Epoch----->56 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.73 .. NELBO: 576.73\n",
      "****************************************************************************************************\n",
      "Epoch: 57 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.2 .. NELBO: 558.2\n",
      "****************************************************************************************************\n",
      "Epoch----->57 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.75 .. NELBO: 576.75\n",
      "****************************************************************************************************\n",
      "Epoch: 58 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.21 .. NELBO: 558.21\n",
      "****************************************************************************************************\n",
      "Epoch----->58 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.77 .. NELBO: 576.77\n",
      "****************************************************************************************************\n",
      "Epoch: 59 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.21 .. NELBO: 558.21\n",
      "****************************************************************************************************\n",
      "Epoch----->59 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.74 .. NELBO: 576.74\n",
      "****************************************************************************************************\n",
      "Epoch: 60 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.24 .. NELBO: 558.24\n",
      "****************************************************************************************************\n",
      "Epoch----->60 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.75 .. NELBO: 576.75\n",
      "****************************************************************************************************\n",
      "Epoch: 61 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.21 .. NELBO: 558.21\n",
      "****************************************************************************************************\n",
      "Epoch----->61 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.76 .. NELBO: 576.76\n",
      "****************************************************************************************************\n",
      "Epoch: 62 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.18 .. NELBO: 558.18\n",
      "****************************************************************************************************\n",
      "Epoch----->62 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.72 .. NELBO: 576.72\n",
      "****************************************************************************************************\n",
      "Epoch: 63 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.22 .. NELBO: 558.22\n",
      "****************************************************************************************************\n",
      "Epoch----->63 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.76 .. NELBO: 576.76\n",
      "****************************************************************************************************\n",
      "Epoch: 64 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.21 .. NELBO: 558.21\n",
      "****************************************************************************************************\n",
      "Epoch----->64 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.75 .. NELBO: 576.75\n",
      "****************************************************************************************************\n",
      "Epoch: 65 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.23 .. NELBO: 558.23\n",
      "****************************************************************************************************\n",
      "Epoch----->65 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.73 .. NELBO: 576.73\n",
      "****************************************************************************************************\n",
      "Epoch: 66 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.17 .. NELBO: 558.17\n",
      "****************************************************************************************************\n",
      "Epoch----->66 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.69 .. NELBO: 576.69\n",
      "****************************************************************************************************\n",
      "Epoch: 67 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.14 .. NELBO: 558.14\n",
      "****************************************************************************************************\n",
      "Epoch----->67 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.68 .. NELBO: 576.68\n",
      "****************************************************************************************************\n",
      "Epoch: 68 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.12 .. NELBO: 558.12\n",
      "****************************************************************************************************\n",
      "Epoch----->68 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.65 .. NELBO: 576.65\n",
      "****************************************************************************************************\n",
      "Epoch: 69 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.07 .. NELBO: 558.07\n",
      "****************************************************************************************************\n",
      "Epoch----->69 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.6 .. NELBO: 576.6\n",
      "****************************************************************************************************\n",
      "Epoch: 70 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.09 .. NELBO: 558.09\n",
      "****************************************************************************************************\n",
      "Epoch----->70 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.62 .. NELBO: 576.62\n",
      "****************************************************************************************************\n",
      "Epoch: 71 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.06 .. NELBO: 558.06\n",
      "****************************************************************************************************\n",
      "Epoch----->71 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.59 .. NELBO: 576.59\n",
      "****************************************************************************************************\n",
      "Epoch: 72 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.03 .. NELBO: 558.03\n",
      "****************************************************************************************************\n",
      "Epoch----->72 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.57 .. NELBO: 576.57\n",
      "****************************************************************************************************\n",
      "Epoch: 73 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.05 .. NELBO: 558.05\n",
      "****************************************************************************************************\n",
      "Epoch----->73 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.58 .. NELBO: 576.58\n",
      "****************************************************************************************************\n",
      "Epoch: 74 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.07 .. NELBO: 558.07\n",
      "****************************************************************************************************\n",
      "Epoch----->74 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.58 .. NELBO: 576.58\n",
      "****************************************************************************************************\n",
      "Epoch: 75 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.07 .. NELBO: 558.07\n",
      "****************************************************************************************************\n",
      "Epoch----->75 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.59 .. NELBO: 576.59\n",
      "****************************************************************************************************\n",
      "Epoch: 76 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.06 .. NELBO: 558.06\n",
      "****************************************************************************************************\n",
      "Epoch----->76 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.58 .. NELBO: 576.58\n",
      "****************************************************************************************************\n",
      "Epoch: 77 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.07 .. NELBO: 558.07\n",
      "****************************************************************************************************\n",
      "Epoch----->77 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.59 .. NELBO: 576.59\n",
      "****************************************************************************************************\n",
      "Epoch: 78 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.12 .. NELBO: 558.12\n",
      "****************************************************************************************************\n",
      "Epoch----->78 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.59 .. NELBO: 576.59\n",
      "****************************************************************************************************\n",
      "Epoch: 79 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.12 .. NELBO: 558.12\n",
      "****************************************************************************************************\n",
      "Epoch----->79 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.6 .. NELBO: 576.6\n",
      "****************************************************************************************************\n",
      "Epoch: 80 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.07 .. NELBO: 558.07\n",
      "****************************************************************************************************\n",
      "Epoch----->80 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.58 .. NELBO: 576.58\n",
      "****************************************************************************************************\n",
      "Epoch: 81 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.05 .. NELBO: 558.05\n",
      "****************************************************************************************************\n",
      "Epoch----->81 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.57 .. NELBO: 576.57\n",
      "****************************************************************************************************\n",
      "Epoch: 82 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.08 .. NELBO: 558.08\n",
      "****************************************************************************************************\n",
      "Epoch----->82 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.57 .. NELBO: 576.57\n",
      "****************************************************************************************************\n",
      "Epoch: 83 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.1 .. NELBO: 558.1\n",
      "****************************************************************************************************\n",
      "Epoch----->83 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.59 .. NELBO: 576.59\n",
      "****************************************************************************************************\n",
      "Epoch: 84 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.06 .. NELBO: 558.06\n",
      "****************************************************************************************************\n",
      "Epoch----->84 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.57 .. NELBO: 576.57\n",
      "****************************************************************************************************\n",
      "Epoch: 85 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.06 .. NELBO: 558.06\n",
      "****************************************************************************************************\n",
      "Epoch----->85 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.56 .. NELBO: 576.56\n",
      "****************************************************************************************************\n",
      "Epoch: 86 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.03 .. NELBO: 558.03\n",
      "****************************************************************************************************\n",
      "Epoch----->86 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.54 .. NELBO: 576.54\n",
      "****************************************************************************************************\n",
      "Epoch: 87 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.05 .. NELBO: 558.05\n",
      "****************************************************************************************************\n",
      "Epoch----->87 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.53 .. NELBO: 576.53\n",
      "****************************************************************************************************\n",
      "Epoch: 88 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.06 .. NELBO: 558.06\n",
      "****************************************************************************************************\n",
      "Epoch----->88 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.53 .. NELBO: 576.53\n",
      "****************************************************************************************************\n",
      "Epoch: 89 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.02 .. NELBO: 558.02\n",
      "****************************************************************************************************\n",
      "Epoch----->89 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.53 .. NELBO: 576.53\n",
      "****************************************************************************************************\n",
      "Epoch: 90 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.99 .. NELBO: 557.99\n",
      "****************************************************************************************************\n",
      "Epoch----->90 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.51 .. NELBO: 576.51\n",
      "****************************************************************************************************\n",
      "Epoch: 91 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.03 .. NELBO: 558.03\n",
      "****************************************************************************************************\n",
      "Epoch----->91 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.51 .. NELBO: 576.51\n",
      "****************************************************************************************************\n",
      "Epoch: 92 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 558.01 .. NELBO: 558.01\n",
      "****************************************************************************************************\n",
      "Epoch----->92 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.45 .. NELBO: 576.45\n",
      "****************************************************************************************************\n",
      "Epoch: 93 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.97 .. NELBO: 557.97\n",
      "****************************************************************************************************\n",
      "Epoch----->93 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.45 .. NELBO: 576.45\n",
      "****************************************************************************************************\n",
      "Epoch: 94 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.89 .. NELBO: 557.89\n",
      "****************************************************************************************************\n",
      "Epoch----->94 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.4 .. NELBO: 576.4\n",
      "****************************************************************************************************\n",
      "Epoch: 95 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.93 .. NELBO: 557.93\n",
      "****************************************************************************************************\n",
      "Epoch----->95 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.42 .. NELBO: 576.42\n",
      "****************************************************************************************************\n",
      "Epoch: 96 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.91 .. NELBO: 557.91\n",
      "****************************************************************************************************\n",
      "Epoch----->96 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.41 .. NELBO: 576.41\n",
      "****************************************************************************************************\n",
      "Epoch: 97 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.93 .. NELBO: 557.93\n",
      "****************************************************************************************************\n",
      "Epoch----->97 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.43 .. NELBO: 576.43\n",
      "****************************************************************************************************\n",
      "Epoch: 98 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.97 .. NELBO: 557.97\n",
      "****************************************************************************************************\n",
      "Epoch----->98 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.44 .. NELBO: 576.44\n",
      "****************************************************************************************************\n",
      "Epoch: 99 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.95 .. NELBO: 557.95\n",
      "****************************************************************************************************\n",
      "Epoch----->99 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.42 .. NELBO: 576.42\n",
      "****************************************************************************************************\n",
      "Epoch: 100 .. batch: 20/32 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 557.93 .. NELBO: 557.93\n",
      "****************************************************************************************************\n",
      "Epoch----->100 .. LR: 0.005 .. KL_theta: 0.0 .. Rec_loss: 576.4 .. NELBO: 576.4\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['like',\n",
       "  'year',\n",
       "  'people',\n",
       "  'know',\n",
       "  'time',\n",
       "  'think',\n",
       "  'go',\n",
       "  'laughter',\n",
       "  'work',\n",
       "  'life'],\n",
       " ['like',\n",
       "  'year',\n",
       "  'people',\n",
       "  'know',\n",
       "  'time',\n",
       "  'go',\n",
       "  'think',\n",
       "  'laughter',\n",
       "  'world',\n",
       "  'life'],\n",
       " ['like',\n",
       "  'people',\n",
       "  'year',\n",
       "  'know',\n",
       "  'time',\n",
       "  'go',\n",
       "  'think',\n",
       "  'laughter',\n",
       "  'life',\n",
       "  'work'],\n",
       " ['like',\n",
       "  'people',\n",
       "  'year',\n",
       "  'know',\n",
       "  'time',\n",
       "  'think',\n",
       "  'go',\n",
       "  'world',\n",
       "  'laughter',\n",
       "  'life'],\n",
       " ['like',\n",
       "  'people',\n",
       "  'year',\n",
       "  'know',\n",
       "  'time',\n",
       "  'think',\n",
       "  'go',\n",
       "  'laughter',\n",
       "  'world',\n",
       "  'work'],\n",
       " ['like',\n",
       "  'people',\n",
       "  'year',\n",
       "  'know',\n",
       "  'time',\n",
       "  'go',\n",
       "  'think',\n",
       "  'work',\n",
       "  'laughter',\n",
       "  'world'],\n",
       " ['like',\n",
       "  'year',\n",
       "  'people',\n",
       "  'know',\n",
       "  'time',\n",
       "  'go',\n",
       "  'think',\n",
       "  'life',\n",
       "  'work',\n",
       "  'laughter'],\n",
       " ['like',\n",
       "  'people',\n",
       "  'year',\n",
       "  'know',\n",
       "  'time',\n",
       "  'think',\n",
       "  'go',\n",
       "  'laughter',\n",
       "  'life',\n",
       "  'work'],\n",
       " ['like',\n",
       "  'year',\n",
       "  'people',\n",
       "  'know',\n",
       "  'time',\n",
       "  'go',\n",
       "  'think',\n",
       "  'laughter',\n",
       "  'life',\n",
       "  'work'],\n",
       " ['like',\n",
       "  'year',\n",
       "  'people',\n",
       "  'know',\n",
       "  'time',\n",
       "  'go',\n",
       "  'think',\n",
       "  'life',\n",
       "  'laughter',\n",
       "  'world']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etm = ETM(use_partitions=False)\n",
    "etm_model = etm.train_model(dataset)\n",
    "etm_model['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cd257a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETM Coherence Score: 0.5501886346578144\n",
      "[['like', 'year', 'people', 'know', 'time', 'think', 'go', 'laughter', 'work', 'life'], ['like', 'year', 'people', 'know', 'time', 'go', 'think', 'laughter', 'world', 'life'], ['like', 'people', 'year', 'know', 'time', 'go', 'think', 'laughter', 'life', 'work'], ['like', 'people', 'year', 'know', 'time', 'think', 'go', 'world', 'laughter', 'life'], ['like', 'people', 'year', 'know', 'time', 'think', 'go', 'laughter', 'world', 'work'], ['like', 'people', 'year', 'know', 'time', 'go', 'think', 'work', 'laughter', 'world'], ['like', 'year', 'people', 'know', 'time', 'go', 'think', 'life', 'work', 'laughter'], ['like', 'people', 'year', 'know', 'time', 'think', 'go', 'laughter', 'life', 'work'], ['like', 'year', 'people', 'know', 'time', 'go', 'think', 'laughter', 'life', 'work'], ['like', 'year', 'people', 'know', 'time', 'go', 'think', 'life', 'laughter', 'world']]\n"
     ]
    }
   ],
   "source": [
    "coherence_model = Coherence(measure=\"c_v\")\n",
    "etm_coherence = coherence_model.score(etm_model)\n",
    "print(f\"ETM Coherence Score: {etm_coherence}\")\n",
    "\n",
    "print(etm_model['topics'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
